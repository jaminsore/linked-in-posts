{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa49ea0d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Pickling and FastText models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b69df",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90307389",
   "metadata": {},
   "source": [
    "[Pickle](https://docs.python.org/3/library/pickle.html#pickle-state) is\n",
    "Python's native serialization module and according the\n",
    "[docs](https://docs.python.org/3/library/pickle.html#pickle-state) it\n",
    "\n",
    "> ... implements binary protocols for serializing and de-serializing a\n",
    "> Python object structure. “Pickling” is the process whereby a Python object\n",
    "> hierarchy is converted into a byte stream, and “unpickling” is the inverse\n",
    "> operation, whereby a byte stream (from a binary file or bytes-like object)\n",
    "> is converted back into an object hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9696b",
   "metadata": {},
   "source": [
    "It is often used as the persistence backend for trained models and other ML\n",
    "artifacts, and is the mechanism used by `multiprocessing` to serialize\n",
    "objects in a `Queue`. But, in many cases objects used in ML rely on compiled\n",
    "extensions written in C, C++, Rust, or Fortran which cannot be pickled. This\n",
    "notebook will demonstrate how to write a custom `__reduce__` method to handle\n",
    "\"unpicklable\" extensions. The example is based on real-world scenario using\n",
    "`fasttext`, but can be applied generally should you find yourself unable to\n",
    "pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64e8bb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855f6c6",
   "metadata": {},
   "source": [
    "Here, we will import the necessary modules and define the convenience\n",
    "functions we will use throughout this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "from typing import Self, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from lorem_text import lorem\n",
    "from fasttext import FastText\n",
    "from sklearn import pipeline, base, cluster\n",
    "\n",
    "\n",
    "def make_fake_data(n_samples: int = 100) -> list[str]:\n",
    "    return [lorem.sentence() for _ in range(n_samples)]\n",
    "\n",
    "\n",
    "def save_pipeline(pipeline: pipeline.Pipeline, filename: str = \"saved_pipeline.pkl\") -> None:\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(pipeline, f)\n",
    "\n",
    "\n",
    "def load_pipeline(filename: str = \"saved_pipeline.pkl\") -> pipeline.Pipeline:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def train_model(corpus: list[str]) -> FastText._FastText:\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        data_file = pathlib.Path(d) / \"training_data.txt\"\n",
    "        data_file.write_text(\"\\n\".join(corpus) + \"\\n\")\n",
    "        model = FastText.train_unsupervised(str(data_file))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ce3d1",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f845cff8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "For the purposes of illustration, we are going to be making a clustering\n",
    "[Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators)\n",
    "with two steps\n",
    "\n",
    "  1. a [FastText](https://fasttext.cc/) embedding transformer step\n",
    "  2. a [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) step\n",
    "\n",
    "We are then going to try to serliaize this `Pipeline` using Python's\n",
    "[pickle](https://docs.python.org/3/library/pickle.html) module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e43f0",
   "metadata": {},
   "source": [
    "### Define the `FastTextTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448da9e9",
   "metadata": {},
   "source": [
    "This is done by implementing the following:\n",
    "\n",
    "  1. inherit from [base.BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn-base-baseestimator) and [base.TransformerMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html#sklearn-base-transformermixin)\n",
    "  2. implement both `fit` and `transform` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ab7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    def __init__(self, model: FastText._FastText):\n",
    "        self.model = model\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, *args: Any, **kwargs: Any) -> Self:\n",
    "        # No-Op\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        text = np.atleast_1d(X)\n",
    "\n",
    "        if text.ndim != 1:\n",
    "            raise ValueError(f\"`X` must be 1-dimensional, received {text.ndim}d data\")\n",
    "\n",
    "        return np.asarray([self.model.get_sentence_vector(s) for s in text.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86235aa2",
   "metadata": {},
   "source": [
    "### Train your FastText model and construct your clustering pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c67783",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_trainig_data = make_fake_data()\n",
    "fasttext_model = train_model(corpus=fasttext_trainig_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7500f8",
   "metadata": {},
   "source": [
    "#### Train your cluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debee3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_training_data = make_fake_data()\n",
    "random_state = np.random.RandomState(1024)  # setting random state for comparability\n",
    "\n",
    "unpicklable_pipeline = pipeline.make_pipeline(\n",
    "    FastTextTransformer(model=fasttext_model), cluster.KMeans(n_clusters=2, n_init=\"auto\", random_state=random_state)\n",
    ").fit(cluster_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23245a8",
   "metadata": {},
   "source": [
    "#### Save your pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3c322",
   "metadata": {},
   "source": [
    "Here we should get\n",
    "> `TypeError: cannot pickle 'fasttext_pybind.fasttext' object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c33244",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pipeline(unpicklable_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca69725",
   "metadata": {},
   "source": [
    "### What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd89c5",
   "metadata": {},
   "source": [
    "`fasttext_pybind.fasttext` is a compiled C++ extension and so cannot be serialized cross-platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f1735",
   "metadata": {},
   "source": [
    "### Are we out of luck?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7163a",
   "metadata": {},
   "source": [
    "At this point we have two choices:\n",
    "\n",
    "  1. accept that we can't use `FastText` models in our pipeline transformations\n",
    "  2. learn some pickling dark magic, and make an object that we can pickle\n",
    "\n",
    "Option $1$ is no fun and wouldn't make for a very interesting post so let's go with option $2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386dad42",
   "metadata": {},
   "source": [
    "### What can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc566419",
   "metadata": {},
   "source": [
    "Fortunately, we have a couple things going for us.\n",
    "\n",
    "  1. `fasttext` provides its own `load_model` and `save_model` methods (see `model` [docs](https://fasttext.cc/docs/en/python-module.html#model-object))\n",
    "  2. Object pickling in Python, like most things, relies on a protocol that we are free to hack away at# %% [markdown]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58aeb15",
   "metadata": {},
   "source": [
    "### The solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff72c4",
   "metadata": {},
   "source": [
    "With this in mind, we simply wrap `fasttext.load_model` and\n",
    "`FastText._FastText.save_model` functionality with Python's pickling\n",
    "protocol. This requires us to extend `FastText._FastText` to include:\n",
    "\n",
    "1. a method to serialize a trained model to `bytes`\n",
    "3. a constructor to instantiate a model from its serialized `bytes` representatino\n",
    "2. override the [\\_\\_reduce\\_\\_](https://docs.python.org/3/library/pickle.html#object.__reduce__) method to return a `tuple` containing\n",
    "  > - A callable object that will be called to create the initial version of the object.\n",
    "  > - A tuple of arguments for the callable object. An empty tuple must be given if the callable does not accept any argument.\n",
    "\n",
    "In our case `__reduce__` will return a `tuple` containing the constructor in\n",
    "(2) and a `tuple` containing the `bytes` of our serialized instance returned\n",
    "from the method in (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef25ea1",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90652a2d",
   "metadata": {},
   "source": [
    "Finally, we land on the following implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tempfile\n",
    "from typing import Callable, Self\n",
    "\n",
    "import fasttext\n",
    "from fasttext import FastText\n",
    "\n",
    "\n",
    "class PicklableFastText(FastText._FastText):  # type: ignore[no-any-unimported]\n",
    "    _tmp_filename = \"model.bin\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model: FastText._FastText) -> Self:  # type: ignore[no-any-unimported]\n",
    "        self = cls.__new__(cls)\n",
    "        self.__dict__.update(model.__dict__)\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, saved_model: bytes | str) -> Self:\n",
    "        if isinstance(saved_model, str) and pathlib.Path(saved_model).exists():\n",
    "            return cls.from_pretrained(fasttext.load_model(saved_model))\n",
    "\n",
    "        if isinstance(saved_model, bytes):\n",
    "            with tempfile.TemporaryDirectory() as d:\n",
    "                model_path = pathlib.Path(d) / cls._tmp_filename\n",
    "                model_path.write_bytes(saved_model)\n",
    "                return cls.load(str(model_path))\n",
    "\n",
    "        raise FileNotFoundError(saved_model)\n",
    "\n",
    "    def serialize(self) -> bytes:\n",
    "        with tempfile.TemporaryDirectory() as d:\n",
    "            model_path = pathlib.Path(d) / self._tmp_filename\n",
    "            self.save_model(str(model_path))\n",
    "            return model_path.read_bytes()\n",
    "\n",
    "    def __reduce__(self) -> tuple[Callable[[bytes | str], Self], tuple[bytes]]:\n",
    "        return self.load, (self.serialize(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d2f6a",
   "metadata": {},
   "source": [
    "#### Update our original pipeline with our `PicklableFastText` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e14a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "picklable_pipeline = pipeline.make_pipeline(\n",
    "    FastTextTransformer(model=PicklableFastText.from_pretrained(fasttext_model)),\n",
    "    cluster.KMeans(n_clusters=2, n_init=\"auto\", random_state=random_state),\n",
    ").fit(cluster_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c4d65",
   "metadata": {},
   "source": [
    "#### Save our new Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pipeline(picklable_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d461d9a",
   "metadata": {},
   "source": [
    "#### Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = lorem.sentence()\n",
    "\n",
    "print(picklable_pipeline.transform([sentence]))\n",
    "print(load_pipeline().transform([sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596acb8",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e7ade",
   "metadata": {},
   "source": [
    "The next time you find yourself unable to pickle an object that you can\n",
    "otherwise save and load, remember to `__reduce__`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
